{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b7c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import asf_search as asf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import contextlib\n",
    "import tempfile\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import getpass\n",
    "import tifffile as tif\n",
    "\n",
    "import xsar\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a25c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './out/sar_segments/'\n",
    "write_folder = '../bouy_survey/1h_survey'\n",
    "result_df_fn = 'result_df'\n",
    "\n",
    "with open(os.path.join(write_folder, result_df_fn),'rb') as f_r:\n",
    "    shore_survey_df = pickle.load(f_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ab92fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:brobeck\n",
      "Password:········\n"
     ]
    }
   ],
   "source": [
    "username = input('Username:')\n",
    "password = getpass.getpass('Password:')\n",
    "session = asf.ASFSession().auth_with_creds(username=username, password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab0efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def sar_download(url):\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:    \n",
    "        asf.download_url(url=url, path=tmp_dir, session=session)       \n",
    "        zip_name = url.split('/')[-1]   \n",
    "        \n",
    "        with ZipFile(os.path.join(tmp_dir, zip_name)) as zf:\n",
    "            zf.extractall(tmp_dir)\n",
    "\n",
    "        yield os.path.join(tmp_dir, zip_name.split('.')[0] + '.SAFE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991611e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tempfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m help(\u001b[43mtempfile\u001b[49m\u001b[38;5;241m.\u001b[39mTemporaryDirectory)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tempfile' is not defined"
     ]
    }
   ],
   "source": [
    "help(tempfile.TemporaryDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6f94223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = shore_survey_df.groupby('sar_url').count().\\\n",
    "       sort_values(by='bouy_file_name', ascending=False).index.to_numpy()\n",
    "\n",
    "filenames = filter(lambda fn: fn.endswith('.tiff'), os.listdir(out_dir))\n",
    "processed_urls = {fn.split('-')[0] for fn in filenames}\n",
    "urls = filter(lambda url: url.split('/')[-1].split('.')[0] not in processed_urls, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "35af04ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 10/10 [56:40<00:00, 340.05s/it]\n",
      "Process SpawnPoolWorker-17:\n",
      "Process SpawnPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/brobeck/opt/anaconda3/envs/era-xsar/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "box_size = 2000 # 2km\n",
    "\n",
    "crop_offsets = {\n",
    "    8:( .5, -.5), 1:( .5, 0), 2:( .5, .5),\n",
    "    7:(  0, -.5), 0:(  0, 0), 3:(  0, .5), \n",
    "    6:(-.5, -.5), 5:(-.5, 0), 4:(-.5, .5)\n",
    "}\n",
    "\n",
    "for url in tqdm(itertools.islice(urls, 10), total=10):\n",
    "    with sar_download(url) as safe_path:\n",
    "        sar_name = url.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        sar_meta = xsar.Sentinel1Meta(safe_path)\n",
    "        sar_ds = xsar.Sentinel1Dataset(sar_meta)\n",
    "        dist = {\n",
    "            'line': int(np.round(box_size / 2 / sar_meta.pixel_line_m)),\n",
    "            'sample': int(np.round(box_size / 2 / sar_meta.pixel_sample_m))\n",
    "        }\n",
    "        \n",
    "        shore_df_part = shore_survey_df[shore_survey_df.sar_url == url]\n",
    "        \n",
    "        for bouy, bouy_df in shore_df_part.groupby('bouy_file_name'):\n",
    "            bouy_name = bouy.split('.')[0]\n",
    "            bouy_lon, bouy_lat = bouy_df.bouy_longitude.mean(), bouy_df.bouy_latitude.mean()\n",
    "            \n",
    "            bouy_line, bouy_sample = sar_ds.ll2coords(bouy_lon, bouy_lat)\n",
    "            \n",
    "            for crop_index, (line_offset, sample_offset) in crop_offsets.items():\n",
    "                offset_line = int(bouy_line + (line_offset * dist['line']))\n",
    "                offset_sample = int(bouy_sample + (sample_offset * dist['sample']))\n",
    "\n",
    "                line_in_range = (0 <= offset_line - dist['line']) and \\\n",
    "                                (offset_line + dist['line'] <= sar_ds.dataset.line[-1].values)\n",
    "\n",
    "                sample_in_range = (0 <= offset_sample - dist['sample']) and \\\n",
    "                                (offset_sample + dist['sample'] <= sar_ds.dataset.sample[-1].values)\n",
    "\n",
    "                if not (line_in_range and sample_in_range): continue\n",
    "\n",
    "                small_sar = sar_ds.dataset.sel(\n",
    "                    line=slice(offset_line - dist['line'], offset_line + dist['line'] - 1),\n",
    "                    sample=slice(offset_sample - dist['sample'], offset_sample + dist['sample'] - 1)\n",
    "                )\n",
    "    \n",
    "                if np.any(small_sar.land_mask): continue\n",
    "                    \n",
    "                out_path = os.path.join(out_dir, f'{sar_name}-{bouy_name}-{crop_index}.tif')\n",
    "                tif.imwrite(out_path, small_sar.sigma0.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_size = 2000 # 2km\n",
    "\n",
    "crop_offsets = {\n",
    "    8:( .5, -.5), 1:( .5, 0), 2:( .5, .5),\n",
    "    7:(  0, -.5), 0:(  0, 0), 3:(  0, .5), \n",
    "    6:(-.5, -.5), 5:(-.5, 0), 4:(-.5, .5)\n",
    "}\n",
    "\n",
    "for url in urls:\n",
    "    with sar_download(url) as safe_path:\n",
    "        sar_name = url.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        # atrack = line, xtrack = sample\n",
    "        sar_meta = xsar.Sentinel1Meta(safe_path)\n",
    "        sar_ds = xsar.Sentinel1Dataset(sar_meta)\n",
    "        dist = {\n",
    "            'atrack': int(np.round(box_size / 2 / sar_meta.pixel_atrack_m)),\n",
    "            'xtrack': int(np.round(box_size / 2 / sar_meta.pixel_xtrack_m))\n",
    "        }\n",
    "        \n",
    "        shore_df_part = shore_survey_df[shore_survey_df.sar_url == url]\n",
    "        \n",
    "        for bouy, bouy_df in shore_df_part.groupby('bouy_file_name'):\n",
    "            bouy_name = bouy.split('.')[0]\n",
    "            bouy_lon, bouy_lat = bouy_df.bouy_longitude.mean(), bouy_df.bouy_latitude.mean()\n",
    "            \n",
    "            bouy_atrack, bouy_xtrack = sar_ds.ll2coords(bouy_lon, bouy_lat)\n",
    "            \n",
    "            for crop_index, (atrack_offset, xtrack_offset) in crop_offsets.items():\n",
    "                offset_atrack = int(bouy_atrack + (atrack_offset * dist['atrack']))\n",
    "                offset_xtrack = int(bouy_xtrack + (xtrack_offset * dist['xtrack']))\n",
    "\n",
    "                atrack_in_range = (0 <= offset_atrack - dist['atrack']) and \\\n",
    "                                (offset_atrack + dist['atrack'] <= sar_ds.dataset.atrack[-1].values)\n",
    "\n",
    "                xtrack_in_range = (0 <= offset_xtrack - dist['xtrack']) and \\\n",
    "                                (offset_xtrack + dist['xtrack'] <= sar_ds.dataset.xtrack[-1].values)\n",
    "\n",
    "                if not (atrack_in_range and xtrack_in_range): continue\n",
    "\n",
    "                small_sar = sar_ds.dataset.sel(\n",
    "                    atrack=slice(offset_atrack - dist['atrack'], offset_atrack + dist['atrack'] - 1),\n",
    "                    xtrack=slice(offset_xtrack - dist['xtrack'], offset_xtrack + dist['xtrack'] - 1)\n",
    "                )\n",
    "\n",
    "                if np.any(small_sar.land_mask): continue\n",
    "                    \n",
    "                out_path = os.path.join(out_dir, f'{sar_name}-{bouy_name}-{crop_index}.tif')\n",
    "                tif.imwrite(out_path, small_sar.sigma0.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7b7e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/vvqgylds56j6qt35q29hhlpm0000gn/T/tmpkg6jvwbj/S1B_IW_GRDH_1SDV_20211210T060601_20211210T060626_029959_039396_3259.SAFE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with sar_download('https://datapool.asf.alaska.edu/GRD_HD/SB/S1B_IW_GRDH_1SDV_20211210T060601_20211210T060626_029959_039396_3259.zip') as safe_path:\n",
    "    print(safe_path)\n",
    "    input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
